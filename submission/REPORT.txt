CS 753 Assignment2: Automatically Recognizing Swahili


Team Members

Aman Kansal  170050027
Ansh Khurana 170050035
Saksham Goel 170050045

Note: 
To run model which uses swahili.big.arpa, set stage=5 in run.sh. By default the script does not run stage 5.

All the results have been obtained on Ubuntu 18.04 LTS and Intel 64 bit processors

Task 1

Optimal Tuned features:
# Begin configuration section.
nj=4
cmd=run.pl
scale_opts="--transition-scale=1.0 --acoustic-scale=0.1 --self-loop-scale=0.1"
num_iters=40    # Number of iterations of training
max_iter_inc=30 # Last iter to increase #Gauss on.
initial_beam=6 # beam used in the first iteration (set smaller to speed up initialization)
regular_beam=10 # beam used after the first iteration
retry_beam=40
totgauss=2000 # Target #Gaussians.
careful=false
boost_silence=1.0 # Factor by which to boost silence likelihoods in alignment
realign_iters="1 2 3 4 5 6 7 8 9 10 12 14 16 18 20 23 26 29 32 35 38"
config= # name of config file.
stage=-4
power=0.5 # exponent to determine number of gaussians from occurrence counts
norm_vars=false # deprecated, prefer --cmvn-opts "--norm-vars=false"
cmvn_opts=  # can be used to add extra options to cmvn.
delta_opts= # can be used to add extra options to add-deltas
# End configuration section.

1. Changed totgauss(TargetGaussians) to 2000
2. Changed power(exponent to determine number of gaussians from occurrence counts) to 0.5

Best result:
WER: 51.10 

## Trial 1
- Gaussians = 2000

### Results
- WER = 53.11

## Trial 2
- Gaussians = 2000
- Iterations = 80
- Iterations till Gaussians increase = 60
- Data alignment passes extrapolated

### Results
- WER = 52.47

## Trial 3
- Gaussians = 2000
- power = 0.5

### Results
- WER = 51.10

## Trial 4
- Gaussians = 2000
- power = 1.0

### Results
- WER = 54.39

## Trial 5
- Gaussians = 2000
- power = 0.5
- Iterations = 80
- Iterations till Gaussians increase = 60
- Data alignment passes extrapolated

### Results
- WER = 53.89

Task 2

(a) Meaning of _B, _E, _I, _S

There are a number of files in data/lang/phones/ that specify various things about the phone set.

As one can see in data/lang/phones/word_boundary.txt, for a phone 'a' we have
a_B begin
a_E end
a_I internal
a_S singleton

The symbols B,E,I,S represent different positions of the phones in the word.

B - starting of word
E - ending of word
I - in between other phones in a word
S - Standalone phones where a word consists of only one phone

Reference: https://kaldi-asr.org/doc/data_prep.html

(b) Phones that start with the special symbol “#

These are disambiguation symbols.

Disambiguations symbols are added to the Lexicon (L) to make the FST determinizable. Disambiguation symbols are the symbols #1, #2, #3 and so on that are inserted at the end of phonemene sequences in the lexicon. When a phoneme sequence is a prefix of another phoneme sequence in the lexicon, or appears in more than one word, it needs to have one of these symbols added after it. These symbols are needed to ensure that the product L o G is determinizable.

To reflect the changes due to disambiguation symbols in the overall decoded graph, self-loops (identity mapping) labelled with disambiguation symbols are added to H and C and disambiguation symbols in the inputs are replaced with epsilons.

Reference: Kaldi documentation - https://kaldi-asr.org/doc/graph.html

Task 3


Optimal arguments for train_delas.sh: 5000 7000


Results for different parameters
numleaves	totgauss	Result
2500		30000 		%WER 48.08 
5000		5000		%WER 46.25
5000		7000		%WER 45.61
5000		9000		%WER 46.53


These paramters are numleaves and totgauss

The two paramters taken by train_deltas.sh are
1. Number of leaves in the decision tree: This parameter defines the number of sets of states that will be sharing the same likelihood model. That is for tying X Triphone models which have the middle monophone as M, if number of leaves are K, these X states (left context/right context..) will be partitioned K sets which share the same GMM parameters.

2. Total number of Gaussians: This is the sum of over all GMM likelihood models over all tied states of the Tied-State HMM model.
That is, if there are x sets of tied-states, where each tied-state set shares a y component GMM - Total number of Gaussians = x*y

Task 4

The following were the various models and corresponding commands
4-gram Kneser-ney (modified) smoothed model
ngram-count -read grams.txt -order 4 -lm demolm.lm -kndiscount1 -kndiscount2 -kndiscount3 -kndiscount4
WER --> 45.98%; logprob= -2901.229 ppl= 410.8843 ppl1= 852.1854

4-gram Kneser-ney (modified) smoothed model with interpolation
ngram-count -read grams.txt -order 4 -lm demolm.lm -kndiscount1 -kndiscount2 -kndiscount3 -kndiscount4 -interpolate 4
WER --> 45.52%; zeroprobs, logprob= -2867.197 ppl= 382.8781 ppl1= 787.3336

written bell smoothing 3-gram
ngram-count -read grams.txt -lm demolm.lm -wbdiscount1 -wbdiscount2 -wbdiscount3
WER --> 458.8945%
ppl= 428.3576 ppl1= 892.9216

2-gram Kneser-ney with interpolation
ngram-count -read grams.txt -order 2 -lm demolm.lm -kndiscount1 -kndiscount2 -interpolate 2
WER--> 46.80%, zeroprobs, logprob= -2927.522 ppl= 433.9179 ppl1= 905.9274


Best Results: 4-gram Kneser-ney (modified) smoothed model with interpolation
WER: 45.52
PPL= 382.8781 PPL1= 787.3336

Task 5

LM rescoring is much faster, albeit not exact so the results are slightly worse.

a)
WER: 41.13

b) Using LM rescoring

WER: 43.33

Task 6

(g) %WER 35.45 [ 257 / 725, 24 ins, 51 del, 182 sub ] exp/tri1/decode_test/werg_13
(l) %WER 78.19 [ 147 / 188, 4 ins, 70 del, 73 sub ] exp/tri1/decode_test/werl_12
(m) %WER 45.83 [ 44 / 96, 0 ins, 14 del, 30 sub ] exp/tri1/decode_test/werm_14
(n) %WER 55.29 [ 47 / 85, 1 ins, 13 del, 33 sub ] exp/tri1/decode_test/wern_10

The highest WER is reported for the  (l) class of audio files. This is expected since l corresponds to very-noisy data (utterance is very noisy) and thus the corresponding output by our model will be inaccurate. The best (lowest) WER has been reported for (g) - good quality audio files (utterance is good), thus the model performs better on this data since it is easier for the model to give correct predictions on less noisy inputs.


Task 7

Assumption: script findword.sh would be run inside the task7 folder.
It requires the following additional scripts in the same folder to function properly: 

task7
├── findword.sh
├── getAudio.py
└── id2phone.py

From recipe/
cd task7/
./findword.sh

Task 8

Uploaded on Kaggle - Team 170050027-170050035-170050045
